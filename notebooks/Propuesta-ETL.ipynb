{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46522e6-e0bc-4bd5-976d-53a214152eb9",
   "metadata": {},
   "source": [
    "# Propuesta ETL – Visitas Web\n",
    "\n",
    "## Metodología\n",
    "El proceso ETL propuesto seguirá un enfoque DataOps, con fases que delimitan las fases de desarrollar, probar y operar los flujos de datos de manera controlada y confiable:\n",
    "\n",
    "| Fase   | Actividades clave | Alcance |\n",
    "|--------|-----------------|---------|\n",
    "| Plan   | Definir requerimientos, KPIs y diseño del pipeline, incluyendo arquitectura, puntos de control, lineamientos y políticas de seguridad y respaldo. | Dentro del alcance |\n",
    "| Develop | Construir los flujos ETL de manera modular a nivel de código | De forma conceptual se describe la estructura de los directorios y los componentes principales del sistema |\n",
    "| Test   | Ejecutar pruebas unitarias y de datos para garantizar la calidad y el cumplimiento de reglas de negocio. | De forma conceptual se describe el entorno y el entregable de la etapa |\n",
    "| Deploy | Promocionar el repositorio de código con el pipeline final y documentación completa. | De forma conceptual se describe el proceso |\n",
    "| Operate | Monitorear la ejecución diaria y asegurar la observabilidad. | De forma conceptual se describe el proceso |\n",
    "\n",
    "---\n",
    "\n",
    "## Contexto y Necesidad\n",
    "Actualmente, los datos de visitas web se almacenan en archivos planos de forma diaria en un directorio predeterminado dentro del servidor SFTP remoto.  \n",
    "Cada archivo TXT almacena eventos diarios de interacción de usuarios en un sitio web, donde cada registro corresponde a un usuario identificado de forma única a través de su email.  \n",
    "\n",
    "Los datos se estructuran en las siguientes categorías conceptuales:\n",
    "- Estado de validez o acceso\n",
    "- Tiempos de interacción\n",
    "- Métricas de engagement\n",
    "- Detalles técnicos y contexto de acceso\n",
    "- Identificadores internos\n",
    "\n",
    "Estos datos permiten analizar el comportamiento y nivel de interacción de los usuarios con fines de monitoreo, segmentación y análisis de engagement.\n",
    "\n",
    "---\n",
    "\n",
    "## Objetivo del Proyecto\n",
    "Diseñar y construir un proceso ETL diario que:\n",
    "- Descargue los archivos `.txt` de visitas web desde el servidor SFTP.\n",
    "- Valide su estructura y consistencia antes de iniciar el procesamiento.\n",
    "- Cargue los registros válidos en las tablas correspondientes de la base de datos.\n",
    "- Almacene los registros inválidos en tablas específicas de errores.\n",
    "- Ejecute puntos de control en etapas críticas del flujo para garantizar trazabilidad y recuperación ante fallos.\n",
    "- Genere respaldo diario de los archivos procesados en el servidor de aplicaciones ETL.\n",
    "- Registre el linaje de los datos y documente cada checkpoint.\n",
    "- Permita monitoreo y alertas automáticas.\n",
    "\n",
    "---\n",
    "\n",
    "## Requerimientos Funcionales\n",
    "\n",
    "### Validación de estructura y layout\n",
    "Se realizó un perfilamiento de los datos para comprender la naturaleza de estos y así poder extraer los patrones en las distintas columnas. Con esta información se definió el layout del archivo config/layout.json, que servirá como guía para crear los schemas de Pydantic y todas las reglas de validación y transformación necesarias.  \n",
    "\n",
    "| Campo            | Tipo esperado | Obligatorio | Validaciones clave |\n",
    "|-----------------|---------------|------------|------------------|\n",
    "| email           | string        | ✅         | Formato válido de correo electrónico |\n",
    "| jk              | string        | ✅         | Debe existir la columna, valor puede ser vacío |\n",
    "| Badmail         | string        | ✅         | Valores esperados: HARD y vacío |\n",
    "| Baja            | string        | ✅         | Valores esperados: SI y vacío |\n",
    "| Fecha envío     | datetime      | ✅         | Formato dd/mm/yyyy hh:mm |\n",
    "| Fecha open      | datetime      | ✅         | Formato dd/mm/yyyy hh:mm (Puede contener -) |\n",
    "| Opens           | integer       | ✅         | ≥ 0 |\n",
    "| Opens virales   | integer       | ✅         | ≥ 0 |\n",
    "| Fecha click     | datetime      | ✅         | Formato dd/mm/yyyy hh:mm (Puede contener -) |\n",
    "| Clicks          | integer       | ✅         | ≥ 0 |\n",
    "| Clicks virales  | integer       | ✅         | ≥ 0 |\n",
    "| Links           | string        | ✅         | Puede contener - |\n",
    "| IPs             | string        | ✅         | Formato n.n.n.n, cada n entre 0 y 255 (Puede contener -) |\n",
    "| Navegadores     | string        | ✅         | Puede estar vacío o contener - |\n",
    "| Plataformas     | string        | ✅         | Puede estar vacío |\n",
    "\n",
    "---\n",
    "\n",
    "### Definición de formato del archivo\n",
    "- Separador: coma (,)\n",
    "- Codificación: UTF-8\n",
    "- Cabecera: incluida\n",
    "- Delimitadores de texto: ninguno o comillas dobles (\")\n",
    "- Fin de línea: `\\n`\n",
    "\n",
    "---\n",
    "\n",
    "## Extracción\n",
    "- Conexión vía SFTP al servidor `8.8.8.8`, directorio `/home/vinkOS/archivosVisitas`.\n",
    "- Descarga diaria de todos los archivos `.txt` no procesados previamente.\n",
    "- Verificación de duplicados mediante bitácora diaria.\n",
    "- Checksum de cada archivo para garantizar integridad.\n",
    "- Reintentos automáticos 3 veces ante fallo de conexión.\n",
    "- Registro en bitácora en el orquetador: nombre de archivo, tamaño, fecha de descarga.\n",
    "\n",
    "---\n",
    "\n",
    "## Transformación\n",
    "\n",
    "### Validación de layout y columnas\n",
    "- 15 columnas esperadas con nombres exactos.\n",
    "\n",
    "### Validación de contenido\n",
    "- Se aplican las reglas y formatos definidos en la tabla del layout, asegurando consistencia en tipos de datos, obligatoriedad y valores permitidos.\n",
    "- Control de versiones de esquemas mediante Pydantic.\n",
    "- Punto de control: staging contiene solo registros válidos; errores en tabla errores.\n",
    "\n",
    "### Políticas de transformación\n",
    "\n",
    "#### 1. Política de Nulos y Manejo de Ausencias (-)\n",
    "| Campo(s) afectado(s) | Valor Origen | Acción de Transformación | Justificación |\n",
    "|----------------------|--------------|-------------------------|---------------|\n",
    "| Contadores (Opens, Opens virales, Clicks, Clicks virales) | - | Se coerciona a 0 (cero) y se convierte a INTEGER | Garantiza que las métricas numéricas se calculen correctamente con valores ≥ 0 |\n",
    "| Fechas opcionales (Fecha open, Fecha click) | - | Se convierte a NULL y se aplica formato DATETIME | Valor estándar para datos no presentes en campos de fecha opcionales |\n",
    "| Campos de texto o bandera (Badmail, Baja, Links, IPs, Navegadores, Plataformas, jk) | - | Se convierte a NULL | Normalización de valores ausentes para la carga en estadística |\n",
    "| Campos obligatorios (email, Fecha envío) | - o formato inválido | Rechazo del registro completo y envío a la tabla errores | Registro no procesable por incumplir estructura o formato mínimo |\n",
    "\n",
    "#### 2. Política de Normalización y Formato\n",
    "- Email: conversión a minúsculas.\n",
    "- Fechas: dd/mm/yyyy HH:mm → YYYY-MM-DD HH:MM:SS.\n",
    "- IPS: formato de ip n.n.n.n\n",
    "- Banderas: normalización de valores categóricos.\n",
    "\n",
    "#### 3. Política de Unicidad y Precedencia\n",
    "- Criterio de duplicado: mismo email y mismos valores en todos los campos.\n",
    "- Regla de resolución: conservar registro con Fecha envío más reciente.\n",
    "\n",
    "#### 4. Política de Integridad de Negocio (Consistencia Temporal)\n",
    "- `Fecha open` ≥ `Fecha envío`.\n",
    "- `Fecha click` ≥ `Fecha open` y ≤ `Fecha envío`.\n",
    "- Violaciones → registro en tabla de errores.\n",
    "\n",
    "---\n",
    "\n",
    "## Carga\n",
    "- Destino final: MySQL (`visitante`, `estadística`, `errores`).\n",
    "- Estrategia:\n",
    "  - `visitante` y `estadística`: incremental.\n",
    "  - `errores`: append.\n",
    "- Validaciones post-carga: registros cargados coinciden con staging.\n",
    "- Backup en zip: `/home/etl/visitas/bckp`.\n",
    "- Eliminación de archivos originales tras backup.\n",
    "\n",
    "---\n",
    "\n",
    "## Operación y Monitoreo\n",
    "- DAG en Airflow con logging detallado.\n",
    "- Métricas y alertas con Prometheus/Grafana.\n",
    "- Linaje de datos documentado con OpenLineage.\n",
    "- Alertas automáticas vía correo o Slack en caso de fallo crítico.\n",
    "\n",
    "---\n",
    "\n",
    "## Requisitos Regulatorios\n",
    "- Emails cifrados en tránsito y reposo (AES256).\n",
    "- Archivos originales conservados 6 meses.\n",
    "- Documentación de linaje completo para auditoría.\n",
    "\n",
    "---\n",
    "\n",
    "## Calidad de Datos\n",
    "- Validación de layout, columnas, emails, fechas y valores numéricos.\n",
    "- Eliminación de duplicados dentro de cada archivo.\n",
    "- Registro de errores en tabla `errores`.\n",
    "- Bitácora diaria y mensual en el orquetador.\n",
    "\n",
    "---\n",
    "\n",
    "## Seguridad y Privacidad\n",
    "- Acceso controlado a SFTP y MySQL mediante credenciales.\n",
    "- Cifrado TLS/AES256.\n",
    "\n",
    "---\n",
    "\n",
    "## Integración de Datos\n",
    "\n",
    "### Clave de Integración Principal\n",
    "- Email → clave única para determinar si crear o actualizar registro.\n",
    "\n",
    "### Tablas de destino\n",
    "\n",
    "#### 1. Tabla `visitante`\n",
    "\n",
    "\n",
    "| Campo DB | Campo Origen | Lógica de Mapeo y Actualización |\n",
    "|----------|--------------|--------------------------------|\n",
    "| email | email | Clave única, minúsculas |\n",
    "| fechaPrimeraVisita | Fecha envío | Inserción inicial |\n",
    "| fechaUltimaVisita | Fecha envío | Actualización con registro procesado |\n",
    "| visitasTotales | Conteo interno | +1 por registro procesado |\n",
    "| visitasAnioActual | Conteo interno | +1, reinicia si cambia año |\n",
    "| visitasMesActual | Conteo interno | +1, reinicia si cambia mes |\n",
    "\n",
    "#### 2. Tabla `estadística`\n",
    "Cada registro válido se inserta como nueva fila.\n",
    "\n",
    "\n",
    "La tabla de `estadística`, los nombres originales del TXT contienen **acentos, espacios y mezcla de mayúsculas y minúsculas**, por lo que no cumplen con un estándar SQL. Para el ETL se asume que la tabla destino en SQL sigue un **formato `snake_case`**, y se realiza el mapeo de los campos del TXT a la tabla SQL como se indica a continuación:\n",
    "\n",
    "| Campo TXT      | Campo SQL      |\n",
    "| -------------- | -------------- |\n",
    "| email          | email          |\n",
    "| jyv            | jyv            |\n",
    "| Badmail        | badmail        |\n",
    "| Baja           | baja           |\n",
    "| Fecha envío    | fecha_envio    |\n",
    "| Fecha open     | fecha_open     |\n",
    "| Opens          | opens          |\n",
    "| Opens virales  | opens_virales  |\n",
    "| Fecha click    | fecha_click    |\n",
    "| Clicks         | clicks         |\n",
    "| Clicks virales | clicks_virales |\n",
    "| Links          | links          |\n",
    "| IPs            | ips            |\n",
    "| Navegadores    | navegadores    |\n",
    "| Plataformas    | plataformas    |\n",
    "\n",
    "#### 3. Tabla `errores`\n",
    "- Agregar registros con información de origen como nuevo registro (append).\n",
    "\n",
    "---\n",
    "\n",
    "## Archivado y Linaje\n",
    "- Backup diario zip: `/home/etl/visitas/bckp`.\n",
    "- Bitácora mensual con archivos procesados y errores.\n",
    "- Linaje documentado: archivo origen → transformaciones → tabla destino.\n",
    "\n",
    "---\n",
    "\n",
    "## Latencia\n",
    "- ETL diario ejecutado fuera de horario pico.\n",
    "- Tiempo estimado: < 1 hora desde extracción hasta carga y backup.\n",
    "\n",
    "---\n",
    "\n",
    "## KPIs\n",
    "- Archivos procesados vs recibidos.\n",
    "- Registros válidos vs registros con errores.\n",
    "- Tiempo de ejecución por etapa.\n",
    "- Alertas generadas y resueltas.\n",
    "\n",
    "---\n",
    "\n",
    "## Licencias heredadas\n",
    "- No se exigen tecnologías específicas.\n",
    "\n",
    "---\n",
    "\n",
    "## Validaciones y Checkpoints ETL\n",
    "\n",
    "### Checkpoint 1: Extracción y Validación de Fuente Cruda\n",
    "- Etapa: Extracción\n",
    "- Transformaciones: Ninguna\n",
    "- Validaciones:\n",
    "  - Completitud y linaje: existencia, nombre, extensión, tamaño >0, hash distinto.\n",
    "  - Exactitud: columnas correctas (15), nombre y orden, layout versionado.\n",
    "  - Archivos con error → `/errores/layout`\n",
    "- Herramientas: Pandas, Pandera, Pydantic.\n",
    "\n",
    "### Checkpoint 2: Limpieza, Conversión y Reglas de Negocio\n",
    "- Etapa: Transformación (Staging)\n",
    "- Limpieza:\n",
    "  - Guion (-) → NULL\n",
    "  - Conversión string → INTEGER/DATETIME\n",
    "  - Email a minúsculas\n",
    "  - Encoding UTF-8\n",
    "- Reglas de negocio:\n",
    "  - Deduplicación por email (fecha envío más reciente)\n",
    "  - Normalización flags categóricos\n",
    "  - Cálculo métricas incrementales\n",
    "- Validaciones:\n",
    "  - Registro a registro: Pydantic\n",
    "  - Dataset completo: Pandera\n",
    "  - Great Expectations para métricas\n",
    "- Integridad:\n",
    "  - Consistencia temporal\n",
    "  - Reglas de negocio, valores permitidos, unicidad\n",
    "\n",
    "### Checkpoint 3: Carga, Post-Carga y Operación\n",
    "- Etapa: Carga\n",
    "- Pre-Carga:\n",
    "  - Validar el esquema con el de la DB\n",
    "  - Ajustar fechas al formato SQL\n",
    "- Validaciones:\n",
    "  - Conteos registros válidos vs errores\n",
    "  - Verificación de integridad de staging\n",
    "  - Backup antes de carga\n",
    "- Post-Carga y Operación:\n",
    "  - Comparar conteos staging vs destino\n",
    "  - Checksum final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d9b4d7-83fa-4de8-a932-0c0d3e4ef1b6",
   "metadata": {},
   "source": [
    "# Diseño\n",
    "\n",
    "\n",
    "\n",
    "## **Arquitectura**\n",
    "\n",
    "<img src=\"../images/Arquitectura.jpg\" alt=\"Descripción de la imagen\">\n",
    "\n",
    "---\n",
    "\n",
    "## **Stack tecnologico**\n",
    "\n",
    "\n",
    "* **Archivos TXT en servidor SFTP:** fuente de datos cruda.\n",
    "* **Python:** motor de transformación y carga de datos.\n",
    "* **Pandas:** manipulación y limpieza de datos.\n",
    "* **Pydantic:** validación de esquema y tipos de datos.\n",
    "* **Great Expectations:** pruebas de calidad y consistencia de datos.\n",
    "* **Airflow:** orquestador del pipeline ETL.\n",
    "* **Prometheus:** recopilación de métricas del proceso.\n",
    "* **Grafana:** visualización de métricas y dashboards.\n",
    "* **Sentry:** monitoreo de errores y alertas.\n",
    "* **Slack:** notificaciones automáticas.\n",
    "* **OpenLineage:** documentación y seguimiento del linaje de datos.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5244ebc9-88c9-47e8-8651-52fe6fa80569",
   "metadata": {},
   "source": [
    "\n",
    "### **Etapa de Desarrollo**\n",
    "\n",
    "En esta etapa realizamos las siguientes actividades:\n",
    "\n",
    "* Se implementa de manera modular el pipeline ETL utilizando **Python** y **Airflow**.\n",
    "* Se crea el **repositorio en GitHub** con la estructura base del proyecto:\n",
    "\n",
    "  * `/dags`: definición de flujos y orquestación.\n",
    "  * `/modules`: módulos de transformación y carga.\n",
    "  * `/schemas`: validación con **Pydantic**.\n",
    "  * `/expectations`: reglas y controles con **Great Expectations**.\n",
    "  * `/integrations`: conexión con **Sentry**, **Slack** y **OpenLineage**.\n",
    "  * `/configs`: variables de entorno y configuraciones generales.\n",
    "* Se generan **DAGs y checkpoints** para validar integridad, consistencia y completitud del flujo.\n",
    "* Se configuran las integraciones con **Sentry** para la gestión de errores y con **Slack** para notificaciones automáticas.\n",
    "* Se habilita **OpenLineage** para registrar el linaje de datos y visualizarlo en **Marqués**.\n",
    "* Se definen las **variables de entorno**, configuraciones de ejecución y contenedores de los servicios del pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapa de Pruebas**\n",
    "\n",
    "En esta etapa realizamos las siguientes actividades:\n",
    "\n",
    "\n",
    "* Se desarrollan **pruebas unitarias** sobre cada módulo en Python para garantizar su correcto funcionamiento.\n",
    "* Se ejecutan **pruebas de datos** con **Pydantic** y **Great Expectations** para validar estructura, tipos de datos y cumplimiento de reglas de negocio.\n",
    "* Se comprueba la **coherencia temporal**, detección de duplicados y consistencia general de los registros.\n",
    "* Se simulan **errores controlados** para validar alertas y notificaciones en **Sentry** y **Slack**.\n",
    "* Se generan **métricas de calidad y estabilidad** que permiten asegurar la confiabilidad del pipeline antes del paso a producción.\n",
    "\n",
    "---\n",
    "\n",
    "### **Etapa de Despliegue y Operación**\n",
    "\n",
    "* Se define la **estrategia de despliegue** utilizando **Google Cloud Platform (GCP)**.\n",
    "* Los servicios se ejecutan en **contenedores Docker**, garantizando portabilidad y aislamiento.\n",
    "* El **aprovisionamiento** de recursos, redes y bases de datos se automatiza con **Terraform**.\n",
    "* Se establece un flujo de **integración y despliegue continuo (CI/CD)** mediante **GitHub Actions**, incluyendo pruebas automáticas, construcción de imágenes y promoción entre entornos.\n",
    "* El **monitoreo de ejecución** se gestiona con **Prometheus** y **Grafana**, mostrando métricas de desempeño, uso de recursos y cumplimiento de tiempos de carga.\n",
    "* Las **alertas de incidentes** se canalizan a **Sentry** y **Slack**, con clasificación por severidad y prioridad de atención.\n",
    "* Se registra el **linaje de datos** con **OpenLineage**, consultable en **Marqués**, para facilitar la trazabilidad y auditoría de los flujos.\n",
    "* Se implementan **respaldos automáticos** de logs y artefactos críticos para asegurar la recuperación y continuidad del servicio.\n",
    "\n",
    "---\n",
    "\n",
    "### **Paso a Producción**\n",
    "\n",
    "Para el paso a producción se debe realizar las siguientes actividades:\n",
    "\n",
    "* Se documenta el proceso completo del pipeline, sus dependencias, puntos de control y variables configurables.\n",
    "* Se definen los **criterios de aceptación** que deben cumplirse antes de liberar el flujo, incluyendo resultados de pruebas unitarias, métricas de calidad y estabilidad de los DAGs.\n",
    "* Se valida el cumplimiento de **políticas de seguridad, acceso y respaldo**, asegurando que los datos sensibles estén encriptados y los permisos correctamente asignados.\n",
    "* Se crea el plan de despriegus con su politica de rollback\n",
    "* Se ejecuta un **despliegue controlado en ambiente de staging**, validando logs, métricas y comportamiento de las alertas antes de su promoción final a producción.\n",
    "\n",
    "---\n",
    "\n",
    "### **Consideraciones para el equipo de Operación**\n",
    "\n",
    "Se valida con el equipo de operaciones que tenga acceso y visibilidad a las herramientas necesarias para monitorear el pipeline y atender incidentes.\n",
    "\n",
    "* El equipo operativo cuenta con acceso a los **tableros de Prometheus y Grafana** para visualizar métricas de ejecución, latencia, volumen de datos y fallos en los DAGs.\n",
    "* Las **alertas y notificaciones** se reciben directamente en los canales designados de **Slack**, con detalle del DAG afectado, hora del incidente y log asociado.\n",
    "* **Sentry** centraliza los errores y excepciones del sistema, indicando el módulo afectado, tipo de fallo y nivel de severidad.\n",
    "* **Marqués** provee visibilidad del linaje de datos, permitiendo rastrear la procedencia, transformaciones y destino final de cada conjunto de datos.\n",
    "* El equipo operativo dispone de una **guía de acción ante incidentes**, que incluye pasos para:\n",
    "\n",
    "  * Verificar estado del DAG y último checkpoint ejecutado.\n",
    "  * Analizar logs en Airflow y Sentry.\n",
    "  * Escalar el incidente al desarrollador responsable si requiere ajuste de código o reconfiguración.\n",
    "* Se realizan **revisiones periódicas** de métricas, alertas y tendencias de fallos para identificar oportunidades de mejora en la operación.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb7f9bb-ff53-4bf7-963b-8959029653b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (google-adk)",
   "language": "python",
   "name": "google-adk"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
